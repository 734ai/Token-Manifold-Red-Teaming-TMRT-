{
  "title": "OpenAI Competition — TMRT Framework submission",
  "id": "muzansano/tmrt-openai-competition-v1",
  "description": "Safety testing is at the heart of progress in AI. Large language models are already drafting contracts, supporting clinicians, and steering autonomous agents; any safety issues or misaligned behaviors can have real consequences. Red-teaming—deliberately probing a system for edge-case misbehavior before those failures appear in the wild—is one of the most effective ways we have to turn raw capability into dependable technology. By improving the methods and widening the community that does this work, we raise the floor for everyone. Every new issue found can become a reusable test, every novel exploit inspires a stronger defense, and shared best-practices spread far beyond any single lab.\n\ngpt-oss-20b is an ideal target to push forward state of the art in red-teaming. This is a powerful new open-weights model released by OpenAI, with extremely efficient reasoning and tool use capability, but is small enough to run on smaller GPUs and can even be run locally. This model has been through extensive internal testing and red-teaming before release, but we believe that more testing is always better. Finding vulnerabilities that might be subtle, long-horizon, or deeply hidden is exactly the kind of challenge that benefits from thousands of independent minds attacking the problem from novel angles and a variety of perspectives.\n\nDuring the hackathon, you and your team will uncover and submit up to five distinct exploits—complete with prompts or prompt-sets, expected outputs, and automated harnesses that demonstrate the failure on demand. You’ll document your discovery process in a concise write-up and (optionally) include any notebooks or code that enabled you to dig deeper. Think of it as building both the map of vulnerabilities and the instruments future red-teamers will use.\n\nCreativity and innovation are encouraged. You can use any method that doesn’t involve training a model or changing its weights for finding issues. Manual probing, creating automated tooling, and developing agentic testing workflows are all excellent directions to consider. You can treat the model as a black box or employ white box methods that take advantage of being able to access the model weights. The important thing is to document and share what you have done in your writeup. Sharing your notebooks and code is also very much encouraged so that the world can reproduce the results and build on them further.\n\nThe judging panel brings together collaborative experts from across the field. We believe that having many different viewpoints and areas of expertise will ensure the best progress for safety research. We also hope that this is the first of several such challenges hosted by different labs, each with collaborative judging panels.\n\nGet started! Take a look at OpenAI's gpt-oss cookbook page to see all the different ways to access the model. Then bring your most creative probes, rigorous engineering, and a desire to make AI safer for everyone. We can’t wait to see what you uncover.",
  "licenses": [
    {
      "name": "CC0-1.0"
    }
  ]
}
